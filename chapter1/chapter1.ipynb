{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f1df8a",
   "metadata": {},
   "source": [
    "# Chapter 1 - Tokenizer and Sampler\n",
    "\n",
    "In this chapter we focus on the outermost layers of a Transformer-based language model. The core of the model is a deep neural network called a Transformer, but your direct interface during text generation is through its input and output.\n",
    "\n",
    "We start with the input and output because they help visualize the end-to-end process. Regardless of how the internal computation works, you ultimately feed text in and receive generated text back. Input and output are not trivial with LMs, unlike simple stdin/stdout, so we explore them first.\n",
    "\n",
    "Thus we dive a little deeper into how these input and output steps work in an LM as the first step of our journey.\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Understand how the input of an LM, i.e. the tokenizer, works  \n",
    "  - Keywords: Byte Pair Encoding (BPE)\n",
    "- Understand how the output of an LM, i.e. the sampler, works  \n",
    "  - Keywords: Softmax, Temperature scaling, Top-K sampling, Top-P sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620eb104",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "When you enter text into ChatGPT, that text isn't how the LLM sees your request because a Transformer network can only understand numbers. We need to translate the text into a sequence of integers. This process is called *tokenization* and the component doing it is the *tokenizer*.\n",
    "\n",
    "### Naive ways to tokenize a text\n",
    "\n",
    "The most naive way to tokenize arbitrary text is splitting by bytes. Because plain text is a sequence of bytes regardless of encoding, you can easily convert any text into a sequence of bytes (0-255).\n",
    "\n",
    "The problem with this approach is the length of the token sequence. We'll see in later chapters that sequence length is one of the most expensive resources of a Transformer network. Because byte-level tokenization has only 256 possible tokens (often called vocabulary size), the sequence ends up longer than methods with a much larger vocabulary.\n",
    "\n",
    "How can we increase the vocabulary size and reduce the sequence length? Another naive approach is to use words as tokens. This doesn't require four tokens for the word `time` like byte-based tokenization, but only a single token representing `time`. However, some languages don't have an easy way to split words, and this method isn't resilient to typos, whereas modern LMs can understand text even with misspellings.\n",
    "\n",
    "### Byte Pair Encoding (BPE)\n",
    "\n",
    "Byte Pair Encoding is a popular method to tokenize text into an arbitrary vocabulary size. We won't dive deep into the implementation details, but the core idea is:\n",
    "\n",
    "- **Training**\n",
    "  1. Split many texts into bytes (like the naive method above)\n",
    "  2. Find the most frequent pair of bytes in the entire corpus\n",
    "  3. Add that pair to the vocabulary as a new token\n",
    "  4. Replace all occurrences of that pair with the new token\n",
    "  5. Repeat 2-4 until the desired vocabulary size is reached\n",
    "- **Tokenizing**\n",
    "  1. Follow the same merge order learned during training\n",
    "\n",
    "BPE is a language-agnostic algorithm but depends on the training data. In the worst case, it can still tokenize a completely unseen text down to individual bytes.\n",
    "\n",
    "Note: the training mentioned here is not training a neural network. It is training new tokens and merge orders from a dataset. Such trained data is usually distributed with the Transformer model itself because they are tightly coupled.\n",
    "\n",
    "### Visualize tokenizer\n",
    "You can visualize the tokenization [here](https://tiktokenizer.vercel.app/?model=Qwen%2FQwen2.5-72B). `Hello` and ` World` (note the leading space) are tokenized to `9707` and `4337`. `Learn` and ` Transformer` become `23824` and `62379`, and `198` represents `\n",
    "`.\n",
    "\n",
    "Lastly, `<|endoftext|>` is a special token indicating the end of text generation and its token id is `151643`. This special token was inserted into the learning dataset of the Transformer so the model knows to emit it when generation should stop.\n",
    "\n",
    "![Tiktokenizer](./tiktokenizer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16965ecf",
   "metadata": {},
   "source": [
    "## Coding\n",
    "\n",
    "Now let's implement a simple tokenization process to see how it works with PyTorch.\n",
    "\n",
    "We use the `transformers` package provided by Hugging Face. This package already contains many pretrained models and tokenizers. While we build the model network using PyTorch primitives, we simply use the pretrained tokenizer for the Qwen3 model as is. If you're interested in implementing BPE yourself, I highly recommend Stanford's CS336 Assignment 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2272ca",
   "metadata": {},
   "source": [
    "First, load the pretrained tokenizer for the Qwen3 model. It has `151,669` vocabulary tokens, which is much larger than the `256` tokens used in byte-based tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cc46fd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocaburary size: 151669\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "vocab_size = tokenizer.vocab_size + len(tokenizer.added_tokens_decoder)\n",
    "print(\"vocaburary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d502630",
   "metadata": {},
   "source": [
    "Next, add a utility function to tokenize a text into a one-dimensional PyTorch tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "78721e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from jaxtyping import Int64\n",
    "\n",
    "def tokenize(text: str) -> Int64[Tensor, \"seq_len\"]:\n",
    "    return tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc2222",
   "metadata": {},
   "source": [
    "The implementation looks a bit awkward, but the output is simple: a 1-D tensor of token IDs (`int`).\n",
    "\n",
    "You can see the output tokens match the ones shown above when visualizing the tokenization because both use the same tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a80ae3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9707, 4337,  198]) Hello World\\n\n",
      "tensor([23824, 62379,   198]) Learn Transformer\\n\n",
      "tensor([151643]) <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "Hello_World = tokenize(\"Hello World\\n\")\n",
    "Learn_Transformer = tokenize(\"Learn Transformer\\n\")\n",
    "EndToken = tokenize(\"<|endoftext|>\")\n",
    "\n",
    "print(Hello_World, \"Hello World\\\\n\")\n",
    "print(Learn_Transformer, \"Learn Transformer\\\\n\")\n",
    "print(EndToken, \"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2bee60",
   "metadata": {},
   "source": [
    "Now we understand what the raw input to a Transformer-based LM looks like.\n",
    "\n",
    "During training the model sees pairs of sequences and expected next tokens:\n",
    "1. `[..., 9707]` (\"\u2026Hello\")\n",
    "   - Expected next token: `4337` (\" World\")\n",
    "2. `[..., 23824]` (\"\u2026Learn\")\n",
    "   - Expected next token: `62379` (\" Transformer\")\n",
    "\n",
    "During inference (text generation) we iteratively feed the predicted token back into the model to generate the next one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e65074",
   "metadata": {},
   "source": [
    "## Sampler\n",
    "\n",
    "On the other side of a Transformer-based language model we need a way to translate the predicted tokens back into text, just like in ChatGPT's responses. This last-mile translation from token id to text is straightforward because we have a full mapping, though we must handle multi-byte characters correctly.\n",
    "\n",
    "However, we first have to decide which single token to produce from the model's output. The Transformer's final output is a vector of weights for the entire vocabulary, not a single token id. Each element represents the confidence for that token given the input text and the generated text so far, so we need to pick exactly one token from this distribution.\n",
    "\n",
    "### Naive method\n",
    "\n",
    "The simplest method is `argmax`. Because a higher weight means a more likely next token, picking the highest value seems logical. `argmax` simply selects the highest value. However, Transformers are stochastic and sometimes the model may be too small, so an `argmax` choice isn't always ideal.\n",
    "\n",
    "### Softmax sampling\n",
    "\n",
    "Instead, we can treat this vector as a probability distribution and sample from it. Because the raw output may contain arbitrary values, including negatives or infinities, we normalize it first. `softmax` is the most common method to convert logits into probabilities between 0 and 1 that sum to 1. After applying `softmax` we can use `multinomial` sampling to pick one token.\n",
    "\n",
    "### Temperature scaling\n",
    "\n",
    "Softmax sampling is the basis of many sampling schemes, but techniques exist to adjust the behavior. One is temperature scaling, where we divide the logits by a temperature value (`logits / temperature`). Lower temperature reduces the variance of the `softmax` distribution\u2014when the temperature approaches zero, sampling becomes almost identical to `argmax`. Higher temperature increases variance and makes lower-probability tokens more likely. High temperature is sometimes considered more creative, while low temperature is more conservative.\n",
    "\n",
    "### Top-K sampling\n",
    "\n",
    "Although a high temperature can generate creative outputs, we typically want to avoid completely irrelevant tokens. Top-K sampling filters out all but the top K tokens before sampling. This keeps the search space fixed and maintains some randomness while avoiding extremely low-probability tokens.\n",
    "\n",
    "### Top-P sampling\n",
    "\n",
    "Top-P is another filtering method. After `softmax`, it keeps only the top P-percentile tokens. Unlike top-K, top-P does not guarantee the number of remaining tokens. If the distribution is sharply peaked it may keep only one or two tokens; if it is broad it may keep many.\n",
    "\n",
    "### Real world usage\n",
    "\n",
    "In practice these three techniques are often used together\u2014typically: 1) temperature scaling, 2) top-K sampling and 3) top-P sampling. The blog posts below describe these techniques in detail:\n",
    "\n",
    "- [Is a Zero Temperature Deterministic?](https://medium.com/google-cloud/is-a-zero-temperature-deterministic-c4a7faef4d20)\n",
    "- [Beyond temperature: Tuning LLM output with top-k and top-p](https://medium.com/google-cloud/beyond-temperature-tuning-llm-output-with-top-k-and-top-p-24c2de5c3b16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8aca1d",
   "metadata": {},
   "source": [
    "## Coding\n",
    "\n",
    "Let's try `softmax` sampling only for now to keep things simple. I'll add an extra section for Temperature + Top-K + Top-P later.\n",
    "\n",
    "For now we assume the output logits vector represents just one token. This isn't realistic because language models typically output a sequence of logits and use the last position as the next token's unnormalized probability distribution. But this simplified setup is enough for now.\n",
    "\n",
    "Then let's create a logits vector as if it's generated by the model:\n",
    "  - The size of the logits vector is the same as the vocabulary size of our tokenizer\n",
    "    - We use `torch.ones()` to create a vector of that size filled with `1`\n",
    "  - \" World\" (`4337`), \" Transformer\" (`62379`) or \"<|endoftext|>\" (`151643`) each with the same probability, and the rest are 0%\n",
    "\n",
    "Note: To represent 0% after `softmax`, we set `-inf` for all other elements because the softmax of `-inf` is always `0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([151669]) tensor([-inf, -inf, -inf,  ..., -inf, -inf, -inf])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "weight = torch.ones(vocab_size) * float(\"-inf\")\n",
    "weight[4337] = 1.0\n",
    "weight[62379] = 1.0\n",
    "weight[151643] = 1.0\n",
    "print(\"logits:\", weight.shape, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b52c952",
   "metadata": {},
   "source": [
    "Let's check the `softmax` values. As expected, all three candidates have 33.33% probability because the logits are `1.0` for all of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444579f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3333, 0.3333, 0.3333]) softmax > 0.0\n",
      "tensor([  4337,  62379, 151643]) softmax > 0.0 indices\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "softmax = torch.softmax(weight, dim=0)\n",
    "print(softmax[softmax > 0.0], \"softmax > 0.0\")\n",
    "print(torch.nonzero(softmax, as_tuple=True)[0], \"softmax > 0.0 indices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9577cb51",
   "metadata": {},
   "source": [
    "Lastly, sample one token by following the softmax probability distribution. Because sampling is stochastic, you'll see different outputs each time you run the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd5c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: ' World'\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "\n",
    "def predict_next_token(logits: Float[Tensor, \"vocab_size\"]) -> str:\n",
    "    softmax = torch.softmax(logits, dim=0)\n",
    "    next_token = torch.multinomial(softmax, num_samples=1).item()\n",
    "    return tokenizer.decode(next_token)\n",
    "\n",
    "print(f\"Next token: '{predict_next_token(weight)}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
