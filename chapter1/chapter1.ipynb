{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f1df8a",
   "metadata": {},
   "source": [
    "# Chapter 1 - Tokenizer and Sampler\n",
    "\n",
    "In this chapter, we'd like to understand the most outer layers of the Transformer based language model. Although the core of the language model is a special deep neural network called Transformer,\n",
    "the input and output interface is the way you interact with the model directly, especially when generating text.\n",
    "\n",
    "The reason why we start from input and output is because it helps you to understand visualizing\n",
    "end to end process easily. No matter how the information is processed in between, it ends up to just inputting a text and outputting a generated text. However, inputting/outputting aren't very trivial with LM unlike stdin/stdout.\n",
    "\n",
    "Thus, we're diving deeper a little bit about the details of how these input/output for LM is working first here. At the end of this chapter, we'll train a very simple model manually to generate tiny fixed text examples although it won't work well.\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Understand how input of LM i.e. Tokenizer works\n",
    "  - Keywords: Byte Pair Encoding (BPE)\n",
    "- Understand how output of LM i.e. Sampler works\n",
    "  - Keywords: Softmax, Temperature scaling, Top-K sampling, Top-P sampling\n",
    "- Understand the basic linear layer model\n",
    "  - Keywords: Linear layer, Matrix multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620eb104",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "When you enter a text into Chat window of ChatGPT, that text isn't the way that LLM sees your request because Transformer network can only understand numbers, not text. So, we need to translate your input text into the sequence of numbers (specifically integers). This process is called \"Tokenization\" and the function doing it is called \"Tokenizer\".\n",
    "\n",
    "### Naive ways to tokenize a text\n",
    "\n",
    "The most naive way to tokenize arbitrary text is spliting by bytes. Because the plain text is actually the sequence if bytes regardless of encoding, you can easily convert any text into a sequence of bytes (0-255).\n",
    "\n",
    "The problem of this approach is the length of the token sequence. We'll see in the following chapters but the sequence length is one of the most expencive resource of Transformer network. Because this method only has 256 types of tokens (often called vocabrary size), the sequence length is longer than the methods having much larger vocabrary size.\n",
    "\n",
    "How to increase the vocabrary size / reduce the sequence length? Another naive approach is using words as tokens. This doesn't require 4 tokens for a word `time` like bytes-as-tokens but just require 1 token that represents `time`. However, some languages don't have easy way to split \"words\". Also, this method is not resilient to typo while the modern LM can understand the text even we type it with misspelling.\n",
    "\n",
    "### Byte Pair Encoding (BPE)\n",
    "\n",
    "Byte Pair Encoding (BPE) is one of the popular methods to tokenize any text into the arbitrary vocabrary size. We won't dive deep into BPE's implementation details, but the core concept is something like below:\n",
    "\n",
    "- Training\n",
    "  1. Split many texts into bytes (like the first method)\n",
    "  2. Find the most frequent pair of bytes from the entire texts\n",
    "  3. Add that pair to the vocaburary as a new token\n",
    "  4. Replace all the pairs to the new token\n",
    "  5. Repeat 2-4 until you reach the desired vocaburary size\n",
    "- Tokenizing\n",
    "  1. Follow the same order of merge process\n",
    "\n",
    "So, BPE is language agnostic algorythm but training data dependent. In the worst case scenario, it could tokenize a text into just a sequence of bytes if the input text is completely new to the training dataset.\n",
    "\n",
    "Note: Training here is not a traing of neural network. It's a training for new tokens and merge orders from a particular dataset. Normally, such a trained data is distributed along with the the Transformer model itself because both are tightly coupled.\n",
    "\n",
    "### Visuzalize tokenizer\n",
    "You can visualize the tokenization [here](https://tiktokenizer.vercel.app/?model=Qwen%2FQwen2.5-72B) like below. `Hello` and ` World` (Note: Including the leading space) are tokenized to `9707` and `4337`. Same for `Learn` and ` Transformer` to `23824` and `62379`. Also, `198` represents `\\n`.\n",
    "\n",
    "Lastly, `<|endoftext|>` is a special token to indicate the end of text generation and it's token id is `151643`. This special text is inserted into the lerning dataset of the Transformer neural network to indicate the end of the text chunk. So, the model knows when to stop the text generation and it predicts this special token, then.\n",
    "\n",
    "![Tiktokenizer](./tiktokenizer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16965ecf",
   "metadata": {},
   "source": [
    "## Coding\n",
    "\n",
    "Now. let's implement the simple tokenization process to see how it really works with PyTorch.\n",
    "\n",
    "We use `transformers` package providedby Hugging Face. This package contains bunch of pretrained models and tokenizers already. While we build the model network using PyTorch premitives, we just use the pretrained tokenizer for Qwen3 model as-is. If you're interested in implementing BPE, I highly recommend you to try Stanford CS336 Assignment 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2272ca",
   "metadata": {},
   "source": [
    "First, load the pretrained tokenize for Qwen3 model. You can see it has `151,669` vocaburaries that is quite larger than `256` (byte based tokenization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cc46fd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocaburary size: 151669\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "vocab_size = tokenizer.vocab_size + len(tokenizer.added_tokens_decoder)\n",
    "print(\"vocaburary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d502630",
   "metadata": {},
   "source": [
    "Next, add a utility function to tokenize a text into PyTorch tensor of one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "78721e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from jaxtyping import Int64\n",
    "\n",
    "def tokenize(text: str) -> Int64[Tensor, \"seq_len\"]:\n",
    "    return tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc2222",
   "metadata": {},
   "source": [
    "The implementation looks awkward, but the output should be easy to understand.\n",
    "\n",
    "You can see the output tokens matches the ones you saw above when visualizing the tokenization because both use the same tokenizer.\n",
    "\n",
    "We keep using these two tiny sequences as the input and output of our models for a while i.e.\n",
    "\n",
    "- If the token is \"Hello\" => Predict \" World\"\n",
    "- If the token is \"Learn\" => Predict \" Transformer\"\n",
    "- If the token is either \" World\" or \" Transformer\" => Predict \"<|endoftext|>\"\n",
    "\n",
    "That's all we want so far. We don't care about any other input text for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a80ae3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9707, 4337]) Hello World\n",
      "tensor([23824, 62379]) Learn Transformer\n",
      "tensor([151643]) <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "Hello_World = tokenize(\"Hello World\")\n",
    "print(Hello_World, \"Hello World\")\n",
    "Learn_Transformer = tokenize(\"Learn Transformer\")\n",
    "print(Learn_Transformer, \"Learn Transformer\")\n",
    "EndToken = tokenize(\"<|endoftext|>\")\n",
    "print(EndToken, \"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e65074",
   "metadata": {},
   "source": [
    "## Sampler\n",
    "\n",
    "On the other side of Transfomer-based language model, we need a way to translate the predicted tokens into text, just like you see as ChatGPT's response. The lastmile translation i.e. from token id to the corresponding text flagment should be straightforward as we have the full mapping of this translation (Note: you have to deal with multi-byte characters well).\n",
    "\n",
    "However, you need one more step to decide what is the single token that Transformer predicts as the next token from its output. To understand this process, let's describe about how Transformer outputs. Transformer's final output is the distribution of weights for all vocaburary. It's not a single token id but a vector that has the same length as the vocabrary size and each element represents the confidence level of the corresponding token id given the input text and the generated text so far. Thus, we need to pick exactly one token based on this distribution.\n",
    "\n",
    "### Naive method\n",
    "\n",
    "The most naive method is called `argmax`. Because the confidence value is higher if the token is highly expected to come next, the highest value's token is the logical choise. `argmax` is a strategy to pick the highest value simply. However, because Transformer is stochastic and sometime the network could be not large enough, `argmax` choice isn't ideal all the time.\n",
    "\n",
    "### Softmax sampling\n",
    "\n",
    "Instead, we can treat this distribution as a probability distribution and sample one token by following the probability, not always pick the highest. Because the Transformer's output could be arbitrary values that might include negative and infinite, we should normalize them. `softmax` is the most popular method to convert the arbitrary numbers (sometime called \"logits\") to the probability values i.e. each value is between 0 and 1 and the sum of all values is 1. In our case, once we apply `softmax` to the logits, we can simply use `multinomial` function to sample one token based on the probability distribution.\n",
    "\n",
    "### Temperature scaling\n",
    "\n",
    "Softmax sampling is the base of the sampling logic but there are several techniques used in the real world sampling to adjust the behavior for some reasons. One method is temperature scaling. This method is simply scale the logis by a single temperature number (`logits / temperature`). This means that the lower the temperature is, the lower variants the `softmax` nomalized distribution becomes. So, if temperature is almost zero, the sampling is almost identical to `argmax`. The higher temperature means higher variant of probability distribution so that there is a high chance to pick lower probability tokens. Sometime people say high temperature is more creative while low temperature is more concervative.\n",
    "\n",
    "### Top-K sampling\n",
    "\n",
    "Although high temperature is great to generate creative outputs, we typically want to avoid completely unnecessary tokens. Top-K sampling is simply filtering out only top K tokens, then sampling. Top-K gives us the fixed size of search pool to maintain some randomness of sampling while avoiding from sampling lower probability tokens.\n",
    "\n",
    "### Top-P sampling\n",
    "\n",
    "Top-P is another filtering method. After `softmax`, it filters out only top P-percentile tokens. Unlike top-K, top-P doesn't gurantee the number of tokens remains. If the distribution is almost delta function, it could only select top 1 or 2. If the distribution is high variant, it would only drop the last 1 or 2 outliers.\n",
    "\n",
    "### Real world usage\n",
    "\n",
    "In the real world, these three techniques are sometime used togather. In such case, people typically do: 1. Temperature scaling, 2. Top-K sampling and 3. Top-P sampling. The blog posts below describe these techniques well, so I highly recommend to read:\n",
    "\n",
    "- [Is a Zero Temperature Deterministic?](https://medium.com/google-cloud/is-a-zero-temperature-deterministic-c4a7faef4d20)\n",
    "- [Beyond temperature: Tuning LLM output with top-k and top-p](https://medium.com/google-cloud/beyond-temperature-tuning-llm-output-with-top-k-and-top-p-24c2de5c3b16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8aca1d",
   "metadata": {},
   "source": [
    "## Coding\n",
    "\n",
    "Let's try `softmax` sampling only for now to minimize the scope. I'll add an extra section to do Temperature + Top-K + Top-P later.\n",
    "\n",
    "For now, we assume the outpt logits is only one token. This isn't realistic, the language models typically output a sequence of tokens and we pick the last logits as the next prediction. But for now, this simple architecture is better.\n",
    "\n",
    "Then, let's create a logits vector as if it's generated by the model:\n",
    "  - The logits's size is the same as the vocaburary size of our tokenizer\n",
    "    - Using `torch.ones()` that generates a vector of the size specified filling `1` everywhere\n",
    "  - \" World\" (`4337`), \" Transformer\" (`62379`) or \"<|endoftext|>\" (`151643`) with the same probability for each and the rest is 0%\n",
    "\n",
    "Note: To represent 0% after `softmax`, we set `-inf` to all the elements (because `softmax` of `-inf` is always `0`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([151669]) tensor([-inf, -inf, -inf,  ..., -inf, -inf, -inf])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "weight = torch.ones(vocab_size) * float(\"-inf\")\n",
    "weight[4337] = 1.0\n",
    "weight[62379] = 1.0\n",
    "weight[151643] = 1.0\n",
    "print(\"logits:\", weight.shape, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b52c952",
   "metadata": {},
   "source": [
    "Let's check `softmax` values. As expected, all the three candidates are 33.33% probability because the logits are `1.0` for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444579f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3333, 0.3333, 0.3333]) softmax > 0.0\n",
      "tensor([  4337,  62379, 151643]) softmax > 0.0 indices\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "softmax = torch.softmax(weight, dim=0)\n",
    "print(softmax[softmax > 0.0], \"softmax > 0.0\")\n",
    "print(torch.nonzero(softmax, as_tuple=True)[0], \"softmax > 0.0 indices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9577cb51",
   "metadata": {},
   "source": [
    "Lastly, sample one token by followin the softmax probability distribution. Because it's sampling, you'll see different outputs whenever you run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd5c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: ' World'\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "\n",
    "def predict_next_token(logits: Float[Tensor, \"vocab_size\"]) -> str:\n",
    "    softmax = torch.softmax(logits, dim=0)\n",
    "    next_token = torch.multinomial(softmax, num_samples=1).item()\n",
    "    return tokenizer.decode(next_token)\n",
    "\n",
    "print(f\"Next token: '{predict_next_token(weight)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70504650",
   "metadata": {},
   "source": [
    "## Liner layer model\n",
    "\n",
    "In this last section, we try to stich all things we've done and mimic it as a manually trained language model that generates one of three tokens randomly.\n",
    "\n",
    "This is not a machine learning but a simple execise to understand how PyTorch's neural network models work.\n",
    "\n",
    "First, let's create `nn.Linear` model with the size of input is `1` and the output is `vocab_size`. The \"linear\" means that this module is just for linear scaling. Althoug it's based on Tensor computation, the mathematical expression is just `Y = AX + B`. In this example, `bias=False` means there is no `B` term, so it's `Y = AX`.\n",
    "\n",
    "`X` is a single element vector which is the token id of the input (remember, we assume only one token at a time.)\n",
    "\n",
    " So far we don't utilize this input information at all.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659e2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: 'CO'\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "model = nn.Linear(1, vocab_size, bias=False)\n",
    "weight = model(tokenize(\"Hello\").float())\n",
    "print(f\"Next token: '{predict_next_token(weight)}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7388dec5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "daf24bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: ' Transformer'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "weight = torch.zeros(vocab_size, 1)\n",
    "weight[4337] = 1.0\n",
    "weight[62379] = 1.0\n",
    "weight[151643] = 1.0\n",
    "\n",
    "model.load_state_dict({\"weight\": weight})\n",
    "\n",
    "logits = model(tokenize(\"Hello\").float())\n",
    "print(f\"Next token: '{predict_next_token(logits)}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
